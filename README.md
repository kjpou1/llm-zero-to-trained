# **LLM: Zero to Trained**

> A fully manual, from-scratch implementation of a Large Language Model (LLM), guided by the book [**Build a Large Language Model from Scratch**](https://github.com/rasbt/LLMs-from-scratch) by Sebastian Raschka.

Unlike simply running code from the [reference repo](https://github.com/rasbt/LLMs-from-scratch.git), this project is about **understanding and re-implementing** every major component â€” tokenizer, model, data loader, training loop â€” by hand.

---
- [**LLM: Zero to Trained**](#llm-zero-to-trained)
  - [ðŸŽ¯ Objective](#-objective)
  - [ðŸ“ Project Structure](#-project-structure)
  - [ðŸ§  Learning Sources](#-learning-sources)
  - [ðŸ—’ï¸ Progress Log](#ï¸-progress-log)
  - [ðŸ”§ Getting Started](#-getting-started)
  - [ðŸ§° Environment Setup (with uv)](#-environment-setup-with-uv)
  - [ðŸš€ CLI: Start Building](#-cli-start-building)
  - [ðŸ“š References \& Inspirations](#-references--inspirations)
  - [ðŸ“œ License](#-license)

---

## ðŸŽ¯ Objective

To develop a working LLM from first principles by:

* Writing each component from scratch in Python
* Following the structure and logic from Raschkaâ€™s book
* Validating ideas through notebooks and experiments
* Building modular, reusable, CLI-driven code

---

## ðŸ“ Project Structure

```
llm-zero-to-trained/
â”œâ”€â”€ src/llmscratch/        â† Modular CLI-driven Python package
â”‚   â”œâ”€â”€ config/            â† Config loader with .env, CLI, YAML support
â”‚   â”œâ”€â”€ models/            â† Core dataclasses and SingletonMeta
â”‚   â”œâ”€â”€ runtime/           â† CLI argument parsing and dispatch
â”‚   â”œâ”€â”€ launch_host.py     â† Entry point for all commands (e.g., preprocess)
â”‚   â””â”€â”€ host.py            â† Command execution coordinator
â”œâ”€â”€ notebooks/             â† Book-aligned exploration notebooks
â”œâ”€â”€ configs/               â† YAML configs for datasets, vocab, etc.
â”œâ”€â”€ datasets/              â† Raw and processed tokenized data
â”œâ”€â”€ pyproject.toml         â† Project metadata and CLI definition
â”œâ”€â”€ README.md              â† This file
â””â”€â”€ PROGRESS.md            â† Running log of milestones
```

---

## ðŸ§  Learning Sources

* ðŸ“˜ *Build a Large Language Model from Scratch* â€“ Sebastian Raschka (2024)
* ðŸ’» [LLMs-from-scratch GitHub Repo](https://github.com/rasbt/LLMs-from-scratch)
* ðŸ§  Karpathyâ€™s [minGPT](https://github.com/karpathy/minGPT) and [nanoGPT](https://github.com/karpathy/nanoGPT) (inspirational, but not reused)

---

## ðŸ—’ï¸ Progress Log

See [PROGRESS.md](./PROGRESS.md) for completed milestones, model checkpoints, and active development notes.

---

## ðŸ”§ Getting Started

```bash
git clone https://github.com/kjpou1/llm-zero-to-trained.git
cd llm-zero-to-trained
```

---

## ðŸ§° Environment Setup (with [uv](https://docs.astral.sh/uv/getting-started/installation/))

```bash
uv venv
source .venv/bin/activate        # macOS/Linux
# OR
.venv\Scripts\activate           # Windows

uv pip install --editable .
uv sync
```

> ðŸ§ª This enables the `llmscratch` CLI from anywhere and installs all dependencies with reproducible locking via `uv`.

---

## ðŸš€ CLI: Start Building

Use the CLI to run modular LLM pipelines:

```bash
llmscratch preprocess --config configs/data_config.yaml
```

More commands like `train`, `sample`, and `evaluate` will follow as the project evolves.

---

## ðŸ“š References & Inspirations

While the architecture is influenced by great projects, all code is original and written from scratch:

* [Raschkaâ€™s LLM Book](https://leanpub.com/llms-from-scratch)
* [minGPT](https://github.com/karpathy/minGPT)
* [nanoGPT](https://github.com/karpathy/nanoGPT)
* [Hugging Face Transformers](https://github.com/huggingface/transformers)

---

## ðŸ“œ License

This project is MIT licensed.
