# LLM: Zero to Trained

This project is a fully manual, from-scratch implementation of a Large Language Model (LLM), guided by the book [**Build a Large Language Model from Scratch**](https://github.com/rasbt/LLMs-from-scratch) by Sebastian Raschka.

Unlike simply running code from the reference repo, the goal here is to **understand and re-implement every major component** â€” tokenizer, model, data loader, training loop â€” by hand.

---

- [LLM: Zero to Trained](#llm-zero-to-trained)
  - [ğŸ¯ Objective](#-objective)
  - [ğŸ“ Project Structure](#-project-structure)
  - [ğŸ§  Learning Sources](#-learning-sources)
  - [ğŸ—’ï¸ Progress Log](#ï¸-progress-log)
  - [ğŸ”§ Getting Started](#-getting-started)
    - [ğŸ§° Environment Setup (with `uv`)](#-environment-setup-with-uv)
    - [ğŸš€ Start Building](#-start-building)
  - [ğŸ“š References \& Inspirations](#-references--inspirations)
  - [ğŸ“œ License](#-license)

---

## ğŸ¯ Objective

To develop a working LLM from first principles by:

- Writing each component from scratch in Python
- Following the structure and logic from the book
- Validating ideas through notebooks and experiments
- Building modular, reusable code

---

## ğŸ“ Project Structure

```

notebooks/      â† Book-aligned exploration notebooks
tokenizer/      â† BPE, WordPiece, and Unigram tokenizer implementations
model/          â† Transformer architecture components
data/           â† Dataset loading, preprocessing, batching utilities
train/          â† Training loop, loss, optimizer, evaluation
experiments/    â† Saved runs, checkpoints, metrics from training runs
configs/        â† YAML configs for models and training (optional)
README.md       â† This file
PROGRESS.md     â† Running log of completed work and learnings

````

---

## ğŸ§  Learning Sources

- [LLMs-from-scratch GitHub Repo](https://github.com/rasbt/LLMs-from-scratch)
- Book: *Build a Large Language Model from Scratch* by Sebastian Raschka

---

## ğŸ—’ï¸ Progress Log

Check [`PROGRESS.md`](PROGRESS.md) for detailed notes and milestones.

---

## ğŸ”§ Getting Started

Clone this repo, then explore and build each component:

```bash
git clone https://github.com/kjpou1/llm-zero-to-trained.git
cd llm-zero-to-trained
```

---

### ğŸ§° Environment Setup (with `uv`)

This project uses [`uv`](https://github.com/astral-sh/uv) for dependency management and reproducible environments.

1. **Install `uv`**
   ğŸ‘‰ Follow the instructions here:
   [https://docs.astral.sh/uv/getting-started/installation/](https://docs.astral.sh/uv/getting-started/installation/)

2. **Create and activate your virtual environment**:

```bash
uv venv
source .venv/bin/activate  # macOS/Linux
# OR
.venv\Scripts\activate     # Windows
```

3. **Sync dependencies**:

```bash
uv sync
```

This installs all required packages and generates a `uv.lock` file for reproducibility.

---

### ğŸš€ Start Building

Begin with:

* `notebooks/01_data_preparation.ipynb` â€“ for exploring the dataset and tokenizer
* `tokenizer/` â€“ to start implementing your own tokenizer logic
* `model/` â€“ when you're ready to dive into Transformers


---

## ğŸ“š References & Inspirations

This project is built entirely from scratch, with a strong emphasis on personal understanding through implementation.

The main learning resource is:

* [*Build a Large Language Model from Scratch* by Sebastian Raschka](https://github.com/rasbt/LLMs-from-scratch) â€” the primary guide followed throughout this project.

Additional projects that may inform future architectural decisions:

* [Karpathyâ€™s minGPT and nanoGPT](https://github.com/karpathy/nanoGPT) â€” minimalist and educational GPT implementations
* [Hugging Face Transformers](https://github.com/huggingface/transformers) â€” a modular, production-ready reference for LLM design

These references serve as conceptual guides, but all implementation in this repository is original and written by hand.

---

## ğŸ“œ License

This project is MIT licensed.

